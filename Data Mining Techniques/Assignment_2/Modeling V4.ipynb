{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import ndcg_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"cleaned_train.csv\")\n",
    "test = pd.read_csv(\"cleaned_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [ \"comp1_rate\", \"comp1_inv\", \"comp2_rate\", \"comp2_inv\", \"comp3_rate\", \"comp3_inv\", \"comp4_rate\", \"comp4_inv\",\n",
    "                \"comp5_rate\", \"comp5_inv\", \"comp6_rate\", \"comp6_inv\", \"comp7_rate\", \"comp7_inv\", \"comp8_rate\", \"comp8_inv\",\n",
    "                \"weekday\", \"month\"]\n",
    "rest = [\"srch_id\", \"site_id\", \"visitor_location_country_id\", \"prop_country_id\", \"prop_id\", \"srch_destination_id\"]\n",
    "\n",
    "target = 'target_score' #click_bool in this case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group split needed for both train-test split and CV\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state = 7)\n",
    "split = splitter.split(train, groups=train['srch_id'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = train.iloc[train_inds]\n",
    "test_df = train.iloc[test_inds]\n",
    "\n",
    "X_train = train_df.drop([target], axis=1)\n",
    "X_test = test_df.drop([target], axis=1)\n",
    "y_train = train_df[target]\n",
    "y_test = test_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **) model with default parameters on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = lgb.LGBMRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 32.9 s, total: 4min 5s\n",
      "Wall time: 1min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_2.fit(X_train, y_train, categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06094215, -0.0292679 , -0.04246334, ..., -0.03407786,\n",
       "       -0.05183003, -0.049999  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "y_pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>position</th>\n",
       "      <th>predictions</th>\n",
       "      <th>predictions_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>0.060942</td>\n",
       "      <td>0.143452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>0.029268</td>\n",
       "      <td>0.105455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>0.042463</td>\n",
       "      <td>0.121284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.046966</td>\n",
       "      <td>0.126686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>0.067535</td>\n",
       "      <td>0.151360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>0.105626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>0.029594</td>\n",
       "      <td>0.105847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.048431</td>\n",
       "      <td>0.128443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0.031605</td>\n",
       "      <td>0.108258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.054236</td>\n",
       "      <td>0.135407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.029268</td>\n",
       "      <td>0.105455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>0.066281</td>\n",
       "      <td>0.149856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0.044097</td>\n",
       "      <td>0.123244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.190343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.062037</td>\n",
       "      <td>0.144765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0.045567</td>\n",
       "      <td>0.125007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.029268</td>\n",
       "      <td>0.105455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.059798</td>\n",
       "      <td>0.142079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>0.123995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.044374</td>\n",
       "      <td>0.123577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>0.042463</td>\n",
       "      <td>0.121284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.049158</td>\n",
       "      <td>0.129316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0.138870</td>\n",
       "      <td>0.236934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.059614</td>\n",
       "      <td>0.141859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0.057379</td>\n",
       "      <td>0.139178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0.048139</td>\n",
       "      <td>0.128093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055087</td>\n",
       "      <td>0.136428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.028977</td>\n",
       "      <td>0.105106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032954</td>\n",
       "      <td>0.109877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>0.019349</td>\n",
       "      <td>0.093556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     srch_id  position  predictions  predictions_n\n",
       "119       12        25     0.060942       0.143452\n",
       "120       12        28     0.029268       0.105455\n",
       "121       12        24     0.042463       0.121284\n",
       "122       12        13     0.046966       0.126686\n",
       "123       12        18     0.067535       0.151360\n",
       "124       12         3     0.029410       0.105626\n",
       "125       12        14     0.029594       0.105847\n",
       "126       12         4     0.048431       0.128443\n",
       "127       12        22     0.031605       0.108258\n",
       "128       12        15     0.054236       0.135407\n",
       "129       12        26     0.029268       0.105455\n",
       "130       12        32     0.066281       0.149856\n",
       "131       12        19     0.044097       0.123244\n",
       "132       12         7     0.100030       0.190343\n",
       "133       12         9     0.062037       0.144765\n",
       "134       12         6     0.045567       0.125007\n",
       "135       12        29     0.029268       0.105455\n",
       "136       12         8     0.059798       0.142079\n",
       "137       12         2     0.044723       0.123995\n",
       "138       12        30     0.044374       0.123577\n",
       "139       12        27     0.042463       0.121284\n",
       "140       12        10     0.049158       0.129316\n",
       "141       12        21     0.138870       0.236934\n",
       "142       12        12     0.059614       0.141859\n",
       "143       12        16     0.057379       0.139178\n",
       "144       12        31     0.048139       0.128093\n",
       "145       12         1     0.055087       0.136428\n",
       "146       12        20     0.028977       0.105106\n",
       "207       25         1     0.032954       0.109877\n",
       "208       25        13     0.019349       0.093556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X_test[\"srch_id\"], pos.iloc[test_inds]], axis=1)\n",
    "df['predictions'] = -y_pred_2\n",
    "df['predictions_n'] = (df['predictions']-df['predictions'].min())/(df['predictions'].max()-df['predictions'].min())\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3905902440826018\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in df['srch_id'].unique():\n",
    "    a1 = [df[df[\"srch_id\"]==i][\"position\"].values]\n",
    "    a2 = [df[df[\"srch_id\"]==i][\"predictions\"].values] \n",
    "    scores.append(ndcg_score(a1, a2, k=5)) \n",
    "print(sum(scores)/len(scores)) #0.39059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Pointwise LGBM regression (no tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 27s, sys: 32.3 s, total: 3min 59s\n",
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fit the same model on whole train data\n",
    "model1.fit(train.drop([target], axis=1), train[target], categorical_feature=rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 31s, sys: 26.5 s, total: 3min 57s\n",
      "Wall time: 1min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.04420504, -0.0808436 , -0.02696308, ..., -0.07440339,\n",
       "       -0.05211163, -0.05319732])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_pred1 = model1.predict(test)\n",
    "y_pred1 # 0,333 NDCG on public leaderboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pointwise LGBM regression (hyperparameters tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=4, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scorer = make_scorer(ndcg_score, k=5, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters with groupKfold\n",
    "lgb_1 = lgb.LGBMRegressor()\n",
    "\n",
    "random_grid_params = {\n",
    "    'learning_rate': [0.05, 0.1, 0.15], \n",
    "    'n_estimators': [80, 100, 110, 120], \n",
    "    'min_child_samples': [17, 20, 23],\n",
    "    'num_leaves': [28, 31, 34],# large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type': [\"gbdt\", \"dart\", \"goss\"], # for better accuracy -> try dart\n",
    "    'max_bin': [255, 300],#large max_bin helps improve accuracy but might slow down training progress\n",
    "    'subsample': [1, 0.9],\n",
    "    'random_state': [42],\n",
    "    'verbose': [1]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(lgb_1, random_grid_params, n_iter=15, scoring=custom_scorer, cv=gss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.839569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.813442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.691823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.692600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.702240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.664507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.666929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.697444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.666874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.766089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.679989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.721086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.679127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.780817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.657359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.695249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.675269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.685326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.670929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.675534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.710056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.737586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.670039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.735676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.663189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.697302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.678689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.697458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.781756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.695855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.683206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.729892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.738931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.745404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.714499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.774484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.725112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.727009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.763313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.729538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.727899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.719800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.714501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.716362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.746388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.704980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.821477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.730349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.722221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.792265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.715929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.719273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.710484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.700971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.718808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.729502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.704201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.677110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.675678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.693342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.920931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54800\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044765\n",
      "CPU times: user 5h 52min 36s, sys: 29min, total: 6h 21min 37s\n",
      "Wall time: 2h 14min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=GroupShuffleSplit(n_splits=4, random_state=None, test_size=0.25,\n",
       "         train_size=None),\n",
       "                   estimator=LGBMRegressor(), n_iter=15,\n",
       "                   param_distributions={'boosting_type': ['gbdt', 'dart',\n",
       "                                                          'goss'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15],\n",
       "                                        'max_bin': [255, 300],\n",
       "                                        'min_child_samples': [17, 20, 23],\n",
       "                                        'n_estimators': [80, 100, 110, 120],\n",
       "                                        'num_leaves': [28, 31, 34],\n",
       "                                        'random_state': [42],\n",
       "                                        'subsample': [1, 0.9], 'verbose': [1]},\n",
       "                   scoring=make_scorer(ndcg5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "random_search.fit(X_train, y_train, groups=X_train['srch_id'], categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'subsample': 1, 'random_state': 42, 'num_leaves': 34, 'n_estimators': 80, 'min_child_samples': 23, 'max_bin': 300, 'learning_rate': 0.1, 'boosting_type': 'dart'}\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_2 = lgb.LGBMRegressor()\n",
    "\n",
    "grid_params = {\n",
    "    'learning_rate': [0.1], \n",
    "    'n_estimators': [75, 80, 85], \n",
    "    'min_child_samples': [23, 25],\n",
    "    'num_leaves': [34],\n",
    "    'boosting_type': ['dart'],\n",
    "    'max_bin': [300],\n",
    "    'subsample': [1],\n",
    "    'random_state': [42],\n",
    "    'verbose': [1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(lgb_2, grid_params, scoring=custom_scorer, cv=gss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.717730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61847\n",
      "[LightGBM] [Info] Number of data points in the train set: 3344833, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044766\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.693088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61789\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346166, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044733\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.673973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61706\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346502, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044745\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.731776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61870\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345982, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044820\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.734519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61847\n",
      "[LightGBM] [Info] Number of data points in the train set: 3344833, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044766\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.731770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61789\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346166, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044733\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.731279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61706\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346502, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044745\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.932101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61870\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345982, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044820\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.694957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61847\n",
      "[LightGBM] [Info] Number of data points in the train set: 3344833, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044766\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.766713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61789\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346166, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044733\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.809531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61706\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346502, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044745\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.695559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61870\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345982, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044820\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.882626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61847\n",
      "[LightGBM] [Info] Number of data points in the train set: 3344833, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044766\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.696699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61789\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346166, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044733\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.690181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61706\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346502, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044745\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.715231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61870\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345982, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044820\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.742902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61847\n",
      "[LightGBM] [Info] Number of data points in the train set: 3344833, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044766\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.719548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61789\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346166, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044733\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.753455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61706\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346502, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044745\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.752869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61870\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345982, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.767947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61847\n",
      "[LightGBM] [Info] Number of data points in the train set: 3344833, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044766\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.801284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61789\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346166, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044733\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.653004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61706\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346502, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044745\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.672726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61870\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345982, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044820\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.900209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54800\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044765\n",
      "CPU times: user 2h 23s, sys: 12min 31s, total: 2h 12min 54s\n",
      "Wall time: 52min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=GroupShuffleSplit(n_splits=4, random_state=None, test_size=0.25,\n",
       "         train_size=None),\n",
       "             estimator=LGBMRegressor(),\n",
       "             param_grid={'boosting_type': ['dart'], 'learning_rate': [0.1],\n",
       "                         'max_bin': [300], 'min_child_samples': [23, 25],\n",
       "                         'n_estimators': [75, 80, 85], 'num_leaves': [34],\n",
       "                         'random_state': [42], 'subsample': [1],\n",
       "                         'verbose': [1]},\n",
       "             scoring=make_scorer(ndcg5))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "grid_search.fit(X_train, y_train, groups=X_train['srch_id'], categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'dart', 'learning_rate': 0.1, 'max_bin': 300, 'min_child_samples': 23, 'n_estimators': 75, 'num_leaves': 34, 'random_state': 42, 'subsample': 1, 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) model with best parameters on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lgb.LGBMRegressor(boosting_type='dart', learning_rate= 0.1, max_bin= 300, min_child_samples= 23, n_estimators= 75, \n",
    "                            num_leaves= 34, random_state= 42, subsample= 1, verbose= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.883699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54800\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044765\n",
      "CPU times: user 4min 57s, sys: 27 s, total: 5min 24s\n",
      "Wall time: 1min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='dart', max_bin=300, min_child_samples=23,\n",
       "              n_estimators=75, num_leaves=34, random_state=42, subsample=1,\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model2.fit(X_train, y_train, categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05891092, -0.02470499, -0.03123527, ..., -0.03825708,\n",
       "       -0.0381674 , -0.04850369])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "y_pred2 = model2.predict(X_test)\n",
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>target_score</th>\n",
       "      <th>predictions</th>\n",
       "      <th>predictions_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058911</td>\n",
       "      <td>0.112061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.048015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031235</td>\n",
       "      <td>0.060242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033468</td>\n",
       "      <td>0.064423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055979</td>\n",
       "      <td>0.106572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.048015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.048015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036114</td>\n",
       "      <td>0.069378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>0.050058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037591</td>\n",
       "      <td>0.072143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.048015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060519</td>\n",
       "      <td>0.115072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>0.063475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084742</td>\n",
       "      <td>0.160428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048968</td>\n",
       "      <td>0.093444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>0.050058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.048015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041055</td>\n",
       "      <td>0.078629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035595</td>\n",
       "      <td>0.068405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035595</td>\n",
       "      <td>0.068405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031235</td>\n",
       "      <td>0.060242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033468</td>\n",
       "      <td>0.064423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118630</td>\n",
       "      <td>0.223878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041055</td>\n",
       "      <td>0.078629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040354</td>\n",
       "      <td>0.077315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033039</td>\n",
       "      <td>0.063620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.091928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.048015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031066</td>\n",
       "      <td>0.059925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018448</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     srch_id  target_score  predictions  predictions_n\n",
       "119       12             0     0.058911       0.112061\n",
       "120       12             0     0.024705       0.048015\n",
       "121       12             0     0.031235       0.060242\n",
       "122       12             0     0.033468       0.064423\n",
       "123       12             0     0.055979       0.106572\n",
       "124       12             0     0.024705       0.048015\n",
       "125       12             0     0.024705       0.048015\n",
       "126       12             0     0.036114       0.069378\n",
       "127       12             0     0.025796       0.050058\n",
       "128       12             0     0.037591       0.072143\n",
       "129       12             0     0.024705       0.048015\n",
       "130       12             0     0.060519       0.115072\n",
       "131       12             0     0.032962       0.063475\n",
       "132       12             0     0.084742       0.160428\n",
       "133       12             0     0.048968       0.093444\n",
       "134       12             0     0.025796       0.050058\n",
       "135       12             0     0.024705       0.048015\n",
       "136       12             0     0.041055       0.078629\n",
       "137       12             0     0.035595       0.068405\n",
       "138       12             0     0.035595       0.068405\n",
       "139       12             0     0.031235       0.060242\n",
       "140       12             0     0.033468       0.064423\n",
       "141       12             0     0.118630       0.223878\n",
       "142       12             0     0.041055       0.078629\n",
       "143       12             0     0.040354       0.077315\n",
       "144       12             0     0.033039       0.063620\n",
       "145       12             1     0.048158       0.091928\n",
       "146       12             0     0.024705       0.048015\n",
       "207       25             0     0.031066       0.059925\n",
       "208       25             0     0.018448       0.036300"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X_test[\"srch_id\"], -y_test], axis=1)\n",
    "df['predictions'] = -y_pred2\n",
    "df['predictions_n'] = (df['predictions']-df['predictions'].min())/(df['predictions'].max()-df['predictions'].min())\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6502984277660459\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in df['srch_id'].unique():\n",
    "#     #t1\n",
    "#     a1 = [df[df[\"srch_id\"]==i][\"target_score\"].values]\n",
    "#     a2 = [df[df[\"srch_id\"]==i][\"predictions\"].values]\n",
    "#     scores.append(ndcg_score(a1, a2, k=5))\n",
    "\n",
    "    #t2 - better so far 0,5\n",
    "    a1 = df[df[\"srch_id\"]==i][\"target_score\"].values\n",
    "    a2 = df[df[\"srch_id\"]==i][\"predictions_n\"].values\n",
    "    scores.append(ndcg5(a1, a2))\n",
    "print(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Model with best parameters on test data trained on training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.207171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 52285\n",
      "[LightGBM] [Info] Number of data points in the train set: 4958347, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044749\n",
      "CPU times: user 5min 9s, sys: 50.1 s, total: 5min 59s\n",
      "Wall time: 2min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='dart', max_bin=300, min_child_samples=23,\n",
       "              n_estimators=75, num_leaves=34, random_state=42, subsample=1,\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model3.fit(train.drop([target], axis=1), train[target], categorical_feature=rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 34.5 s, total: 2min 6s\n",
      "Wall time: 1min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03317082, -0.07022708, -0.02995487, ..., -0.06802567,\n",
       "       -0.04854744, -0.04361254])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_pred3 = model3.predict(test)\n",
    "y_pred3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Pointwise LGBM regression (hyperparameters tuned manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=4, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scorer = make_scorer(ndcg_score, k=5, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FITTED\n",
      "1114548\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([      0,       1,       2,       3,       4,       5,       6,\\n                  7,       8,       9,\\n            ...\\n            4461196, 4461197, 4461198, 4461199, 4461200, 4461201, 4461202,\\n            4461203, 4461204, 4461205],\\n           dtype='int64', length=1114548)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         self._validate_read_indexer(\n\u001b[0m\u001b[1;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([      0,       1,       2,       3,       4,       5,       6,\\n                  7,       8,       9,\\n            ...\\n            4461196, 4461197, 4461198, 4461199, 4461200, 4461201, 4461202,\\n            4461203, 4461204, 4461205],\\n           dtype='int64', length=1114548)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "it_sc = []\n",
    "for train_idx, test_idx in gss.split(X_train, y_train, groups=X_train['srch_id']):\n",
    "    lgb_1 = lgb.LGBMRegressor(learning_rate=0.05)\n",
    "    lgb_1.fit(X_train.iloc[train_idx], y_train.iloc[train_idx], categorical_feature=rest)\n",
    "    print(\"FITTED\")\n",
    "    pred = lgb_1.predict(X_train.iloc[test_idx])\n",
    "    print(len(pred))\n",
    "    df = pd.concat([X_train[\"srch_id\"].iloc[test_idx], pos.iloc[test_idx]], axis=1)\n",
    "    print(\"FAULT preds\")\n",
    "    df['predictions'] = -pred\n",
    "    scores = []\n",
    "    for i in df['srch_id'].unique():\n",
    "        a1 = [df[df[\"srch_id\"]==i][\"position\"].values]\n",
    "        a2 = [df[df[\"srch_id\"]==i][\"predictions\"].values] \n",
    "        scores.append(ndcg_score(a1, a2, k=5)) \n",
    "    print(sum(scores)/len(scores))\n",
    "    it_sc.append(sum(scores)/len(scores))\n",
    "print(it_sc)\n",
    "print(sum(it_sc)/len(it_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters with groupKfold\n",
    "lgb_1 = lgb.LGBMRegressor()\n",
    "\n",
    "random_grid_params = {\n",
    "    'learning_rate': [0.05, 0.1, 0.15], \n",
    "    'n_estimators': [80, 100, 110, 120], \n",
    "    'min_child_samples': [17, 20, 23],\n",
    "    'num_leaves': [28, 31, 34],# large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type': [\"gbdt\", \"dart\", \"goss\"], # for better accuracy -> try dart\n",
    "    'max_bin': [255, 300],#large max_bin helps improve accuracy but might slow down training progress\n",
    "    'subsample': [1, 0.9],\n",
    "    'random_state': [42],\n",
    "    'verbose': [1]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(lgb_1, random_grid_params, n_iter=15, scoring=custom_scorer, cv=gss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.839569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.813442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.691823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.692600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.702240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.664507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.666929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.697444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.666874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.766089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.679989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.721086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.679127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.780817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.657359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.695249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.675269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.685326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.670929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.675534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.710056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.737586 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.670039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.735676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.663189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.697302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.678689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.697458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.781756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.695855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.683206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.729892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.738931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.745404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.714499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.774484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.725112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.727009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.763313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.729538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.727899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.719800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.714501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.716362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.746388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.704980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.821477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.730349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.722221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.792265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.715929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.719273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.710484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61533\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.700971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.718808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61152\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.729502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61327\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.704201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61960\n",
      "[LightGBM] [Info] Number of data points in the train set: 3346997, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044774\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.677110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61648\n",
      "[LightGBM] [Info] Number of data points in the train set: 3345559, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044846\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.675678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61645\n",
      "[LightGBM] [Info] Number of data points in the train set: 3343685, number of used features: 99\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044803\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.693342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 61749\n",
      "[LightGBM] [Info] Number of data points in the train set: 3347754, number of used features: 100\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -0.044727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.920931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54800\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score -0.044765\n",
      "CPU times: user 5h 52min 36s, sys: 29min, total: 6h 21min 37s\n",
      "Wall time: 2h 14min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=GroupShuffleSplit(n_splits=4, random_state=None, test_size=0.25,\n",
       "         train_size=None),\n",
       "                   estimator=LGBMRegressor(), n_iter=15,\n",
       "                   param_distributions={'boosting_type': ['gbdt', 'dart',\n",
       "                                                          'goss'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15],\n",
       "                                        'max_bin': [255, 300],\n",
       "                                        'min_child_samples': [17, 20, 23],\n",
       "                                        'n_estimators': [80, 100, 110, 120],\n",
       "                                        'num_leaves': [28, 31, 34],\n",
       "                                        'random_state': [42],\n",
       "                                        'subsample': [1, 0.9], 'verbose': [1]},\n",
       "                   scoring=make_scorer(ndcg5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "random_search.fit(X_train, y_train, groups=X_train['srch_id'], categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Listwise: LGBMRanker with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ = -y_train\n",
    "y_test_ = -y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = lgb.LGBMRanker(objective=\"lambdarank\", metric=\"ndcg\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['prop_country_id', 'prop_id', 'site_id', 'srch_destination_id', 'srch_id', 'visitor_location_country_id']\n",
      "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.939136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54305\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@5: 0.299461\tvalid_1's ndcg@5: 0.292636\n",
      "[2]\ttraining's ndcg@5: 0.339943\tvalid_1's ndcg@5: 0.325278\n",
      "[3]\ttraining's ndcg@5: 0.352922\tvalid_1's ndcg@5: 0.333621\n",
      "[4]\ttraining's ndcg@5: 0.36205\tvalid_1's ndcg@5: 0.340558\n",
      "[5]\ttraining's ndcg@5: 0.367696\tvalid_1's ndcg@5: 0.346121\n",
      "[6]\ttraining's ndcg@5: 0.372135\tvalid_1's ndcg@5: 0.348248\n",
      "[7]\ttraining's ndcg@5: 0.37652\tvalid_1's ndcg@5: 0.351114\n",
      "[8]\ttraining's ndcg@5: 0.379403\tvalid_1's ndcg@5: 0.352944\n",
      "[9]\ttraining's ndcg@5: 0.382513\tvalid_1's ndcg@5: 0.354109\n",
      "[10]\ttraining's ndcg@5: 0.384195\tvalid_1's ndcg@5: 0.356146\n",
      "[11]\ttraining's ndcg@5: 0.387085\tvalid_1's ndcg@5: 0.357555\n",
      "[12]\ttraining's ndcg@5: 0.389873\tvalid_1's ndcg@5: 0.359794\n",
      "[13]\ttraining's ndcg@5: 0.392433\tvalid_1's ndcg@5: 0.360516\n",
      "[14]\ttraining's ndcg@5: 0.393826\tvalid_1's ndcg@5: 0.360487\n",
      "[15]\ttraining's ndcg@5: 0.397919\tvalid_1's ndcg@5: 0.363979\n",
      "[16]\ttraining's ndcg@5: 0.39977\tvalid_1's ndcg@5: 0.365685\n",
      "[17]\ttraining's ndcg@5: 0.400871\tvalid_1's ndcg@5: 0.366001\n",
      "[18]\ttraining's ndcg@5: 0.403141\tvalid_1's ndcg@5: 0.366552\n",
      "[19]\ttraining's ndcg@5: 0.404682\tvalid_1's ndcg@5: 0.366711\n",
      "[20]\ttraining's ndcg@5: 0.406064\tvalid_1's ndcg@5: 0.366995\n",
      "[21]\ttraining's ndcg@5: 0.407294\tvalid_1's ndcg@5: 0.367148\n",
      "[22]\ttraining's ndcg@5: 0.408342\tvalid_1's ndcg@5: 0.367164\n",
      "[23]\ttraining's ndcg@5: 0.41118\tvalid_1's ndcg@5: 0.369475\n",
      "[24]\ttraining's ndcg@5: 0.41214\tvalid_1's ndcg@5: 0.369639\n",
      "[25]\ttraining's ndcg@5: 0.413536\tvalid_1's ndcg@5: 0.37054\n",
      "[26]\ttraining's ndcg@5: 0.415496\tvalid_1's ndcg@5: 0.371322\n",
      "[27]\ttraining's ndcg@5: 0.416467\tvalid_1's ndcg@5: 0.371569\n",
      "[28]\ttraining's ndcg@5: 0.417954\tvalid_1's ndcg@5: 0.371365\n",
      "[29]\ttraining's ndcg@5: 0.419884\tvalid_1's ndcg@5: 0.372244\n",
      "[30]\ttraining's ndcg@5: 0.421168\tvalid_1's ndcg@5: 0.372486\n",
      "[31]\ttraining's ndcg@5: 0.42253\tvalid_1's ndcg@5: 0.373218\n",
      "[32]\ttraining's ndcg@5: 0.424188\tvalid_1's ndcg@5: 0.374424\n",
      "[33]\ttraining's ndcg@5: 0.42489\tvalid_1's ndcg@5: 0.374508\n",
      "[34]\ttraining's ndcg@5: 0.425894\tvalid_1's ndcg@5: 0.375227\n",
      "[35]\ttraining's ndcg@5: 0.426807\tvalid_1's ndcg@5: 0.375707\n",
      "[36]\ttraining's ndcg@5: 0.428073\tvalid_1's ndcg@5: 0.376192\n",
      "[37]\ttraining's ndcg@5: 0.429546\tvalid_1's ndcg@5: 0.375997\n",
      "[38]\ttraining's ndcg@5: 0.430552\tvalid_1's ndcg@5: 0.376615\n",
      "[39]\ttraining's ndcg@5: 0.431312\tvalid_1's ndcg@5: 0.376947\n",
      "[40]\ttraining's ndcg@5: 0.432126\tvalid_1's ndcg@5: 0.376667\n",
      "[41]\ttraining's ndcg@5: 0.432707\tvalid_1's ndcg@5: 0.377008\n",
      "[42]\ttraining's ndcg@5: 0.433896\tvalid_1's ndcg@5: 0.377125\n",
      "[43]\ttraining's ndcg@5: 0.435067\tvalid_1's ndcg@5: 0.377954\n",
      "[44]\ttraining's ndcg@5: 0.436064\tvalid_1's ndcg@5: 0.377682\n",
      "[45]\ttraining's ndcg@5: 0.43691\tvalid_1's ndcg@5: 0.377954\n",
      "[46]\ttraining's ndcg@5: 0.437732\tvalid_1's ndcg@5: 0.37844\n",
      "[47]\ttraining's ndcg@5: 0.43875\tvalid_1's ndcg@5: 0.378274\n",
      "[48]\ttraining's ndcg@5: 0.439632\tvalid_1's ndcg@5: 0.378636\n",
      "[49]\ttraining's ndcg@5: 0.440883\tvalid_1's ndcg@5: 0.379279\n",
      "[50]\ttraining's ndcg@5: 0.44179\tvalid_1's ndcg@5: 0.379186\n",
      "[51]\ttraining's ndcg@5: 0.442398\tvalid_1's ndcg@5: 0.37906\n",
      "[52]\ttraining's ndcg@5: 0.443705\tvalid_1's ndcg@5: 0.379241\n",
      "[53]\ttraining's ndcg@5: 0.444582\tvalid_1's ndcg@5: 0.379286\n",
      "[54]\ttraining's ndcg@5: 0.445039\tvalid_1's ndcg@5: 0.379455\n",
      "[55]\ttraining's ndcg@5: 0.445866\tvalid_1's ndcg@5: 0.380243\n",
      "[56]\ttraining's ndcg@5: 0.447151\tvalid_1's ndcg@5: 0.380137\n",
      "[57]\ttraining's ndcg@5: 0.447876\tvalid_1's ndcg@5: 0.380194\n",
      "[58]\ttraining's ndcg@5: 0.448689\tvalid_1's ndcg@5: 0.380495\n",
      "[59]\ttraining's ndcg@5: 0.449281\tvalid_1's ndcg@5: 0.380738\n",
      "[60]\ttraining's ndcg@5: 0.450461\tvalid_1's ndcg@5: 0.381084\n",
      "[61]\ttraining's ndcg@5: 0.450965\tvalid_1's ndcg@5: 0.380933\n",
      "[62]\ttraining's ndcg@5: 0.451767\tvalid_1's ndcg@5: 0.381039\n",
      "[63]\ttraining's ndcg@5: 0.452842\tvalid_1's ndcg@5: 0.38198\n",
      "[64]\ttraining's ndcg@5: 0.453302\tvalid_1's ndcg@5: 0.381626\n",
      "[65]\ttraining's ndcg@5: 0.453934\tvalid_1's ndcg@5: 0.382116\n",
      "[66]\ttraining's ndcg@5: 0.45461\tvalid_1's ndcg@5: 0.382232\n",
      "[67]\ttraining's ndcg@5: 0.455489\tvalid_1's ndcg@5: 0.382434\n",
      "[68]\ttraining's ndcg@5: 0.456243\tvalid_1's ndcg@5: 0.38243\n",
      "[69]\ttraining's ndcg@5: 0.456791\tvalid_1's ndcg@5: 0.382317\n",
      "[70]\ttraining's ndcg@5: 0.457566\tvalid_1's ndcg@5: 0.382534\n",
      "[71]\ttraining's ndcg@5: 0.458105\tvalid_1's ndcg@5: 0.382585\n",
      "[72]\ttraining's ndcg@5: 0.458509\tvalid_1's ndcg@5: 0.38311\n",
      "[73]\ttraining's ndcg@5: 0.459384\tvalid_1's ndcg@5: 0.382916\n",
      "[74]\ttraining's ndcg@5: 0.460413\tvalid_1's ndcg@5: 0.382991\n",
      "[75]\ttraining's ndcg@5: 0.461082\tvalid_1's ndcg@5: 0.383133\n",
      "[76]\ttraining's ndcg@5: 0.461534\tvalid_1's ndcg@5: 0.382778\n",
      "[77]\ttraining's ndcg@5: 0.462008\tvalid_1's ndcg@5: 0.383118\n",
      "[78]\ttraining's ndcg@5: 0.463065\tvalid_1's ndcg@5: 0.383748\n",
      "[79]\ttraining's ndcg@5: 0.463634\tvalid_1's ndcg@5: 0.384151\n",
      "[80]\ttraining's ndcg@5: 0.464354\tvalid_1's ndcg@5: 0.384122\n",
      "[81]\ttraining's ndcg@5: 0.464802\tvalid_1's ndcg@5: 0.384137\n",
      "[82]\ttraining's ndcg@5: 0.465441\tvalid_1's ndcg@5: 0.384113\n",
      "[83]\ttraining's ndcg@5: 0.466152\tvalid_1's ndcg@5: 0.384359\n",
      "[84]\ttraining's ndcg@5: 0.466867\tvalid_1's ndcg@5: 0.384906\n",
      "[85]\ttraining's ndcg@5: 0.467511\tvalid_1's ndcg@5: 0.385112\n",
      "[86]\ttraining's ndcg@5: 0.46786\tvalid_1's ndcg@5: 0.38507\n",
      "[87]\ttraining's ndcg@5: 0.468556\tvalid_1's ndcg@5: 0.385192\n",
      "[88]\ttraining's ndcg@5: 0.469232\tvalid_1's ndcg@5: 0.384946\n",
      "[89]\ttraining's ndcg@5: 0.469819\tvalid_1's ndcg@5: 0.385385\n",
      "[90]\ttraining's ndcg@5: 0.470432\tvalid_1's ndcg@5: 0.385679\n",
      "[91]\ttraining's ndcg@5: 0.470858\tvalid_1's ndcg@5: 0.385439\n",
      "[92]\ttraining's ndcg@5: 0.471585\tvalid_1's ndcg@5: 0.385524\n",
      "[93]\ttraining's ndcg@5: 0.472212\tvalid_1's ndcg@5: 0.385662\n",
      "[94]\ttraining's ndcg@5: 0.47291\tvalid_1's ndcg@5: 0.385421\n",
      "[95]\ttraining's ndcg@5: 0.473226\tvalid_1's ndcg@5: 0.385519\n",
      "[96]\ttraining's ndcg@5: 0.473866\tvalid_1's ndcg@5: 0.385753\n",
      "[97]\ttraining's ndcg@5: 0.474551\tvalid_1's ndcg@5: 0.385311\n",
      "[98]\ttraining's ndcg@5: 0.474959\tvalid_1's ndcg@5: 0.385523\n",
      "[99]\ttraining's ndcg@5: 0.475746\tvalid_1's ndcg@5: 0.385946\n",
      "[100]\ttraining's ndcg@5: 0.476284\tvalid_1's ndcg@5: 0.385938\n",
      "CPU times: user 4min 41s, sys: 40.5 s, total: 5min 21s\n",
      "Wall time: 2min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRanker(metric='ndcg', objective='lambdarank', verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model6.fit(X_train, y_train_, eval_set=[(X_train, y_train_), (X_test, y_test_)], eval_group=[X_train['srch_id'].value_counts(sort=False).sort_index(), X_test['srch_id'].value_counts(sort=False).sort_index()], group=X_train['srch_id'].value_counts(sort=False).sort_index(),\n",
    "            eval_at=5,categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46175809, -0.7501605 , -0.54465082, ..., -0.26479785,\n",
       "        0.05585781,  0.35997866])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_6 = model6.predict(X_test)\n",
    "y_pred_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([X_test[\"srch_id\"], pos.iloc[test_inds]], axis=1)\n",
    "df['predictions'] = y_pred_6\n",
    "# df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3724792233297772\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in df['srch_id'].unique():\n",
    "    a1 = [df[df[\"srch_id\"]==i][\"position\"].values]\n",
    "    a2 = [df[df[\"srch_id\"]==i][\"predictions\"].values]\n",
    "    scores.append(ndcg_score(a1, a2, k=5))\n",
    "print(sum(scores)/len(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46165524,  0.88704194, -0.1051241 , ...,  0.01261689,\n",
       "        0.06631362,  0.17210367])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred6 = model6.predict(test)\n",
    "y_pred6 #scored 0,36 on public leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual try parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model66 = lgb.LGBMRanker(objective=\"lambdarank\", metric=\"ndcg\", verbose=1, num_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['prop_country_id', 'prop_id', 'site_id', 'srch_destination_id', 'srch_id', 'visitor_location_country_id']\n",
      "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.031486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54305\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@5: 0.299461\tvalid_1's ndcg@5: 0.292636\n",
      "[2]\ttraining's ndcg@5: 0.339943\tvalid_1's ndcg@5: 0.325278\n",
      "[3]\ttraining's ndcg@5: 0.352922\tvalid_1's ndcg@5: 0.333621\n",
      "[4]\ttraining's ndcg@5: 0.36205\tvalid_1's ndcg@5: 0.340558\n",
      "[5]\ttraining's ndcg@5: 0.367696\tvalid_1's ndcg@5: 0.346121\n",
      "[6]\ttraining's ndcg@5: 0.372135\tvalid_1's ndcg@5: 0.348248\n",
      "[7]\ttraining's ndcg@5: 0.37652\tvalid_1's ndcg@5: 0.351114\n",
      "[8]\ttraining's ndcg@5: 0.379403\tvalid_1's ndcg@5: 0.352944\n",
      "[9]\ttraining's ndcg@5: 0.382513\tvalid_1's ndcg@5: 0.354109\n",
      "[10]\ttraining's ndcg@5: 0.384195\tvalid_1's ndcg@5: 0.356146\n",
      "[11]\ttraining's ndcg@5: 0.387085\tvalid_1's ndcg@5: 0.357555\n",
      "[12]\ttraining's ndcg@5: 0.389873\tvalid_1's ndcg@5: 0.359794\n",
      "[13]\ttraining's ndcg@5: 0.392433\tvalid_1's ndcg@5: 0.360516\n",
      "[14]\ttraining's ndcg@5: 0.393826\tvalid_1's ndcg@5: 0.360487\n",
      "[15]\ttraining's ndcg@5: 0.397919\tvalid_1's ndcg@5: 0.363979\n",
      "[16]\ttraining's ndcg@5: 0.39977\tvalid_1's ndcg@5: 0.365685\n",
      "[17]\ttraining's ndcg@5: 0.400871\tvalid_1's ndcg@5: 0.366001\n",
      "[18]\ttraining's ndcg@5: 0.403141\tvalid_1's ndcg@5: 0.366552\n",
      "[19]\ttraining's ndcg@5: 0.404682\tvalid_1's ndcg@5: 0.366711\n",
      "[20]\ttraining's ndcg@5: 0.406064\tvalid_1's ndcg@5: 0.366995\n",
      "[21]\ttraining's ndcg@5: 0.407294\tvalid_1's ndcg@5: 0.367148\n",
      "[22]\ttraining's ndcg@5: 0.408342\tvalid_1's ndcg@5: 0.367164\n",
      "[23]\ttraining's ndcg@5: 0.41118\tvalid_1's ndcg@5: 0.369475\n",
      "[24]\ttraining's ndcg@5: 0.41214\tvalid_1's ndcg@5: 0.369639\n",
      "[25]\ttraining's ndcg@5: 0.413536\tvalid_1's ndcg@5: 0.37054\n",
      "[26]\ttraining's ndcg@5: 0.415496\tvalid_1's ndcg@5: 0.371322\n",
      "[27]\ttraining's ndcg@5: 0.416467\tvalid_1's ndcg@5: 0.371569\n",
      "[28]\ttraining's ndcg@5: 0.417954\tvalid_1's ndcg@5: 0.371365\n",
      "[29]\ttraining's ndcg@5: 0.419884\tvalid_1's ndcg@5: 0.372244\n",
      "[30]\ttraining's ndcg@5: 0.421168\tvalid_1's ndcg@5: 0.372486\n",
      "[31]\ttraining's ndcg@5: 0.42253\tvalid_1's ndcg@5: 0.373218\n",
      "[32]\ttraining's ndcg@5: 0.424188\tvalid_1's ndcg@5: 0.374424\n",
      "[33]\ttraining's ndcg@5: 0.42489\tvalid_1's ndcg@5: 0.374508\n",
      "[34]\ttraining's ndcg@5: 0.425894\tvalid_1's ndcg@5: 0.375227\n",
      "[35]\ttraining's ndcg@5: 0.426807\tvalid_1's ndcg@5: 0.375707\n",
      "[36]\ttraining's ndcg@5: 0.428073\tvalid_1's ndcg@5: 0.376192\n",
      "[37]\ttraining's ndcg@5: 0.429546\tvalid_1's ndcg@5: 0.375997\n",
      "[38]\ttraining's ndcg@5: 0.430552\tvalid_1's ndcg@5: 0.376615\n",
      "[39]\ttraining's ndcg@5: 0.431312\tvalid_1's ndcg@5: 0.376947\n",
      "[40]\ttraining's ndcg@5: 0.432126\tvalid_1's ndcg@5: 0.376667\n",
      "[41]\ttraining's ndcg@5: 0.432707\tvalid_1's ndcg@5: 0.377008\n",
      "[42]\ttraining's ndcg@5: 0.433896\tvalid_1's ndcg@5: 0.377125\n",
      "[43]\ttraining's ndcg@5: 0.435067\tvalid_1's ndcg@5: 0.377954\n",
      "[44]\ttraining's ndcg@5: 0.436064\tvalid_1's ndcg@5: 0.377682\n",
      "[45]\ttraining's ndcg@5: 0.43691\tvalid_1's ndcg@5: 0.377954\n",
      "[46]\ttraining's ndcg@5: 0.437732\tvalid_1's ndcg@5: 0.37844\n",
      "[47]\ttraining's ndcg@5: 0.43875\tvalid_1's ndcg@5: 0.378274\n",
      "[48]\ttraining's ndcg@5: 0.439632\tvalid_1's ndcg@5: 0.378636\n",
      "[49]\ttraining's ndcg@5: 0.440883\tvalid_1's ndcg@5: 0.379279\n",
      "[50]\ttraining's ndcg@5: 0.44179\tvalid_1's ndcg@5: 0.379186\n",
      "[51]\ttraining's ndcg@5: 0.442398\tvalid_1's ndcg@5: 0.37906\n",
      "[52]\ttraining's ndcg@5: 0.443705\tvalid_1's ndcg@5: 0.379241\n",
      "[53]\ttraining's ndcg@5: 0.444582\tvalid_1's ndcg@5: 0.379286\n",
      "[54]\ttraining's ndcg@5: 0.445039\tvalid_1's ndcg@5: 0.379455\n",
      "[55]\ttraining's ndcg@5: 0.445866\tvalid_1's ndcg@5: 0.380243\n",
      "[56]\ttraining's ndcg@5: 0.447151\tvalid_1's ndcg@5: 0.380137\n",
      "[57]\ttraining's ndcg@5: 0.447876\tvalid_1's ndcg@5: 0.380194\n",
      "[58]\ttraining's ndcg@5: 0.448689\tvalid_1's ndcg@5: 0.380495\n",
      "[59]\ttraining's ndcg@5: 0.449281\tvalid_1's ndcg@5: 0.380738\n",
      "[60]\ttraining's ndcg@5: 0.450461\tvalid_1's ndcg@5: 0.381084\n",
      "[61]\ttraining's ndcg@5: 0.450965\tvalid_1's ndcg@5: 0.380933\n",
      "[62]\ttraining's ndcg@5: 0.451767\tvalid_1's ndcg@5: 0.381039\n",
      "[63]\ttraining's ndcg@5: 0.452842\tvalid_1's ndcg@5: 0.38198\n",
      "[64]\ttraining's ndcg@5: 0.453302\tvalid_1's ndcg@5: 0.381626\n",
      "[65]\ttraining's ndcg@5: 0.453934\tvalid_1's ndcg@5: 0.382116\n",
      "[66]\ttraining's ndcg@5: 0.45461\tvalid_1's ndcg@5: 0.382232\n",
      "[67]\ttraining's ndcg@5: 0.455489\tvalid_1's ndcg@5: 0.382434\n",
      "[68]\ttraining's ndcg@5: 0.456243\tvalid_1's ndcg@5: 0.38243\n",
      "[69]\ttraining's ndcg@5: 0.456791\tvalid_1's ndcg@5: 0.382317\n",
      "[70]\ttraining's ndcg@5: 0.457566\tvalid_1's ndcg@5: 0.382534\n",
      "[71]\ttraining's ndcg@5: 0.458105\tvalid_1's ndcg@5: 0.382585\n",
      "[72]\ttraining's ndcg@5: 0.458509\tvalid_1's ndcg@5: 0.38311\n",
      "[73]\ttraining's ndcg@5: 0.459384\tvalid_1's ndcg@5: 0.382916\n",
      "[74]\ttraining's ndcg@5: 0.460413\tvalid_1's ndcg@5: 0.382991\n",
      "[75]\ttraining's ndcg@5: 0.461082\tvalid_1's ndcg@5: 0.383133\n",
      "[76]\ttraining's ndcg@5: 0.461534\tvalid_1's ndcg@5: 0.382778\n",
      "[77]\ttraining's ndcg@5: 0.462008\tvalid_1's ndcg@5: 0.383118\n",
      "[78]\ttraining's ndcg@5: 0.463065\tvalid_1's ndcg@5: 0.383748\n",
      "[79]\ttraining's ndcg@5: 0.463634\tvalid_1's ndcg@5: 0.384151\n",
      "[80]\ttraining's ndcg@5: 0.464354\tvalid_1's ndcg@5: 0.384122\n",
      "[81]\ttraining's ndcg@5: 0.464802\tvalid_1's ndcg@5: 0.384137\n",
      "[82]\ttraining's ndcg@5: 0.465441\tvalid_1's ndcg@5: 0.384113\n",
      "[83]\ttraining's ndcg@5: 0.466152\tvalid_1's ndcg@5: 0.384359\n",
      "[84]\ttraining's ndcg@5: 0.466867\tvalid_1's ndcg@5: 0.384906\n",
      "[85]\ttraining's ndcg@5: 0.467511\tvalid_1's ndcg@5: 0.385112\n",
      "[86]\ttraining's ndcg@5: 0.46786\tvalid_1's ndcg@5: 0.38507\n",
      "[87]\ttraining's ndcg@5: 0.468556\tvalid_1's ndcg@5: 0.385192\n",
      "[88]\ttraining's ndcg@5: 0.469232\tvalid_1's ndcg@5: 0.384946\n",
      "[89]\ttraining's ndcg@5: 0.469819\tvalid_1's ndcg@5: 0.385385\n",
      "[90]\ttraining's ndcg@5: 0.470432\tvalid_1's ndcg@5: 0.385679\n",
      "[91]\ttraining's ndcg@5: 0.470858\tvalid_1's ndcg@5: 0.385439\n",
      "[92]\ttraining's ndcg@5: 0.471585\tvalid_1's ndcg@5: 0.385524\n",
      "[93]\ttraining's ndcg@5: 0.472212\tvalid_1's ndcg@5: 0.385662\n",
      "[94]\ttraining's ndcg@5: 0.47291\tvalid_1's ndcg@5: 0.385421\n",
      "[95]\ttraining's ndcg@5: 0.473226\tvalid_1's ndcg@5: 0.385519\n",
      "[96]\ttraining's ndcg@5: 0.473866\tvalid_1's ndcg@5: 0.385753\n",
      "[97]\ttraining's ndcg@5: 0.474551\tvalid_1's ndcg@5: 0.385311\n",
      "[98]\ttraining's ndcg@5: 0.474959\tvalid_1's ndcg@5: 0.385523\n",
      "[99]\ttraining's ndcg@5: 0.475746\tvalid_1's ndcg@5: 0.385946\n",
      "[100]\ttraining's ndcg@5: 0.476284\tvalid_1's ndcg@5: 0.385938\n",
      "[101]\ttraining's ndcg@5: 0.476637\tvalid_1's ndcg@5: 0.38583\n",
      "[102]\ttraining's ndcg@5: 0.477381\tvalid_1's ndcg@5: 0.385699\n",
      "[103]\ttraining's ndcg@5: 0.477796\tvalid_1's ndcg@5: 0.38556\n",
      "[104]\ttraining's ndcg@5: 0.478558\tvalid_1's ndcg@5: 0.386196\n",
      "[105]\ttraining's ndcg@5: 0.47902\tvalid_1's ndcg@5: 0.386405\n",
      "[106]\ttraining's ndcg@5: 0.479403\tvalid_1's ndcg@5: 0.386461\n",
      "[107]\ttraining's ndcg@5: 0.480028\tvalid_1's ndcg@5: 0.386429\n",
      "[108]\ttraining's ndcg@5: 0.480628\tvalid_1's ndcg@5: 0.386707\n",
      "[109]\ttraining's ndcg@5: 0.481277\tvalid_1's ndcg@5: 0.386936\n",
      "[110]\ttraining's ndcg@5: 0.481618\tvalid_1's ndcg@5: 0.387093\n",
      "[111]\ttraining's ndcg@5: 0.482134\tvalid_1's ndcg@5: 0.387156\n",
      "[112]\ttraining's ndcg@5: 0.482425\tvalid_1's ndcg@5: 0.387072\n",
      "[113]\ttraining's ndcg@5: 0.483101\tvalid_1's ndcg@5: 0.387356\n",
      "[114]\ttraining's ndcg@5: 0.483591\tvalid_1's ndcg@5: 0.387485\n",
      "[115]\ttraining's ndcg@5: 0.484271\tvalid_1's ndcg@5: 0.3876\n",
      "[116]\ttraining's ndcg@5: 0.484701\tvalid_1's ndcg@5: 0.387532\n",
      "[117]\ttraining's ndcg@5: 0.485119\tvalid_1's ndcg@5: 0.387674\n",
      "[118]\ttraining's ndcg@5: 0.485649\tvalid_1's ndcg@5: 0.387727\n",
      "[119]\ttraining's ndcg@5: 0.486135\tvalid_1's ndcg@5: 0.387958\n",
      "[120]\ttraining's ndcg@5: 0.486681\tvalid_1's ndcg@5: 0.387835\n",
      "[121]\ttraining's ndcg@5: 0.487107\tvalid_1's ndcg@5: 0.387659\n",
      "[122]\ttraining's ndcg@5: 0.48778\tvalid_1's ndcg@5: 0.387649\n",
      "[123]\ttraining's ndcg@5: 0.488187\tvalid_1's ndcg@5: 0.387709\n",
      "[124]\ttraining's ndcg@5: 0.488698\tvalid_1's ndcg@5: 0.387674\n",
      "[125]\ttraining's ndcg@5: 0.489399\tvalid_1's ndcg@5: 0.387908\n",
      "[126]\ttraining's ndcg@5: 0.489911\tvalid_1's ndcg@5: 0.387687\n",
      "[127]\ttraining's ndcg@5: 0.490188\tvalid_1's ndcg@5: 0.387658\n",
      "[128]\ttraining's ndcg@5: 0.49085\tvalid_1's ndcg@5: 0.387898\n",
      "[129]\ttraining's ndcg@5: 0.491434\tvalid_1's ndcg@5: 0.388162\n",
      "[130]\ttraining's ndcg@5: 0.492043\tvalid_1's ndcg@5: 0.388075\n",
      "[131]\ttraining's ndcg@5: 0.492408\tvalid_1's ndcg@5: 0.388116\n",
      "[132]\ttraining's ndcg@5: 0.492711\tvalid_1's ndcg@5: 0.387926\n",
      "[133]\ttraining's ndcg@5: 0.493224\tvalid_1's ndcg@5: 0.387669\n",
      "[134]\ttraining's ndcg@5: 0.493705\tvalid_1's ndcg@5: 0.387161\n",
      "[135]\ttraining's ndcg@5: 0.494394\tvalid_1's ndcg@5: 0.387191\n",
      "[136]\ttraining's ndcg@5: 0.494758\tvalid_1's ndcg@5: 0.38744\n",
      "[137]\ttraining's ndcg@5: 0.495359\tvalid_1's ndcg@5: 0.387446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138]\ttraining's ndcg@5: 0.495693\tvalid_1's ndcg@5: 0.386988\n",
      "[139]\ttraining's ndcg@5: 0.496066\tvalid_1's ndcg@5: 0.387077\n",
      "[140]\ttraining's ndcg@5: 0.496495\tvalid_1's ndcg@5: 0.387055\n",
      "[141]\ttraining's ndcg@5: 0.496934\tvalid_1's ndcg@5: 0.386898\n",
      "[142]\ttraining's ndcg@5: 0.497655\tvalid_1's ndcg@5: 0.387003\n",
      "[143]\ttraining's ndcg@5: 0.498067\tvalid_1's ndcg@5: 0.386671\n",
      "[144]\ttraining's ndcg@5: 0.498404\tvalid_1's ndcg@5: 0.386446\n",
      "[145]\ttraining's ndcg@5: 0.498834\tvalid_1's ndcg@5: 0.386529\n",
      "[146]\ttraining's ndcg@5: 0.499142\tvalid_1's ndcg@5: 0.386503\n",
      "[147]\ttraining's ndcg@5: 0.499526\tvalid_1's ndcg@5: 0.386362\n",
      "[148]\ttraining's ndcg@5: 0.499939\tvalid_1's ndcg@5: 0.386376\n",
      "[149]\ttraining's ndcg@5: 0.500508\tvalid_1's ndcg@5: 0.38647\n",
      "[150]\ttraining's ndcg@5: 0.50094\tvalid_1's ndcg@5: 0.386645\n",
      "[151]\ttraining's ndcg@5: 0.50128\tvalid_1's ndcg@5: 0.386563\n",
      "[152]\ttraining's ndcg@5: 0.501859\tvalid_1's ndcg@5: 0.386751\n",
      "[153]\ttraining's ndcg@5: 0.502314\tvalid_1's ndcg@5: 0.386871\n",
      "[154]\ttraining's ndcg@5: 0.502616\tvalid_1's ndcg@5: 0.387047\n",
      "[155]\ttraining's ndcg@5: 0.502952\tvalid_1's ndcg@5: 0.386764\n",
      "[156]\ttraining's ndcg@5: 0.503267\tvalid_1's ndcg@5: 0.386399\n",
      "[157]\ttraining's ndcg@5: 0.503547\tvalid_1's ndcg@5: 0.386321\n",
      "[158]\ttraining's ndcg@5: 0.504138\tvalid_1's ndcg@5: 0.386519\n",
      "[159]\ttraining's ndcg@5: 0.504615\tvalid_1's ndcg@5: 0.386551\n",
      "[160]\ttraining's ndcg@5: 0.504976\tvalid_1's ndcg@5: 0.386842\n",
      "[161]\ttraining's ndcg@5: 0.505316\tvalid_1's ndcg@5: 0.38682\n",
      "[162]\ttraining's ndcg@5: 0.505646\tvalid_1's ndcg@5: 0.386837\n",
      "[163]\ttraining's ndcg@5: 0.506142\tvalid_1's ndcg@5: 0.387096\n",
      "[164]\ttraining's ndcg@5: 0.506547\tvalid_1's ndcg@5: 0.387094\n",
      "[165]\ttraining's ndcg@5: 0.50684\tvalid_1's ndcg@5: 0.387037\n",
      "[166]\ttraining's ndcg@5: 0.507452\tvalid_1's ndcg@5: 0.387086\n",
      "[167]\ttraining's ndcg@5: 0.507895\tvalid_1's ndcg@5: 0.387109\n",
      "[168]\ttraining's ndcg@5: 0.508376\tvalid_1's ndcg@5: 0.386893\n",
      "[169]\ttraining's ndcg@5: 0.508717\tvalid_1's ndcg@5: 0.386629\n",
      "[170]\ttraining's ndcg@5: 0.509117\tvalid_1's ndcg@5: 0.386817\n",
      "[171]\ttraining's ndcg@5: 0.509598\tvalid_1's ndcg@5: 0.387064\n",
      "[172]\ttraining's ndcg@5: 0.510065\tvalid_1's ndcg@5: 0.386926\n",
      "[173]\ttraining's ndcg@5: 0.510379\tvalid_1's ndcg@5: 0.38748\n",
      "[174]\ttraining's ndcg@5: 0.510612\tvalid_1's ndcg@5: 0.387318\n",
      "[175]\ttraining's ndcg@5: 0.510949\tvalid_1's ndcg@5: 0.387738\n",
      "[176]\ttraining's ndcg@5: 0.511293\tvalid_1's ndcg@5: 0.387756\n",
      "[177]\ttraining's ndcg@5: 0.511664\tvalid_1's ndcg@5: 0.387534\n",
      "[178]\ttraining's ndcg@5: 0.512\tvalid_1's ndcg@5: 0.387291\n",
      "[179]\ttraining's ndcg@5: 0.512276\tvalid_1's ndcg@5: 0.387358\n",
      "[180]\ttraining's ndcg@5: 0.512673\tvalid_1's ndcg@5: 0.387315\n",
      "[181]\ttraining's ndcg@5: 0.512983\tvalid_1's ndcg@5: 0.387305\n",
      "[182]\ttraining's ndcg@5: 0.513419\tvalid_1's ndcg@5: 0.387149\n",
      "[183]\ttraining's ndcg@5: 0.513761\tvalid_1's ndcg@5: 0.387422\n",
      "[184]\ttraining's ndcg@5: 0.514233\tvalid_1's ndcg@5: 0.387236\n",
      "[185]\ttraining's ndcg@5: 0.514761\tvalid_1's ndcg@5: 0.387571\n",
      "[186]\ttraining's ndcg@5: 0.515148\tvalid_1's ndcg@5: 0.387641\n",
      "[187]\ttraining's ndcg@5: 0.515447\tvalid_1's ndcg@5: 0.387455\n",
      "[188]\ttraining's ndcg@5: 0.515779\tvalid_1's ndcg@5: 0.387675\n",
      "[189]\ttraining's ndcg@5: 0.516055\tvalid_1's ndcg@5: 0.387624\n",
      "[190]\ttraining's ndcg@5: 0.516355\tvalid_1's ndcg@5: 0.387776\n",
      "[191]\ttraining's ndcg@5: 0.516654\tvalid_1's ndcg@5: 0.387817\n",
      "[192]\ttraining's ndcg@5: 0.516993\tvalid_1's ndcg@5: 0.387721\n",
      "[193]\ttraining's ndcg@5: 0.517366\tvalid_1's ndcg@5: 0.387647\n",
      "[194]\ttraining's ndcg@5: 0.517747\tvalid_1's ndcg@5: 0.387936\n",
      "[195]\ttraining's ndcg@5: 0.518174\tvalid_1's ndcg@5: 0.387826\n",
      "[196]\ttraining's ndcg@5: 0.518554\tvalid_1's ndcg@5: 0.387782\n",
      "[197]\ttraining's ndcg@5: 0.518873\tvalid_1's ndcg@5: 0.387873\n",
      "[198]\ttraining's ndcg@5: 0.519329\tvalid_1's ndcg@5: 0.387599\n",
      "[199]\ttraining's ndcg@5: 0.519647\tvalid_1's ndcg@5: 0.387631\n",
      "[200]\ttraining's ndcg@5: 0.519905\tvalid_1's ndcg@5: 0.387411\n",
      "CPU times: user 8min 26s, sys: 56.3 s, total: 9min 22s\n",
      "Wall time: 3min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRanker(metric='ndcg', num_iterations=200, objective='lambdarank', verbose=1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model66.fit(X_train, y_train_, eval_set=[(X_train, y_train_), (X_test, y_test_)], eval_group=[X_train['srch_id'].value_counts(sort=False).sort_index(), X_test['srch_id'].value_counts(sort=False).sort_index()], group=X_train['srch_id'].value_counts(sort=False).sort_index(),\n",
    "            eval_at=5,categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49892916, -0.77827307, -0.48718981, ..., -0.38726199,\n",
       "        0.22071585,  0.40486471])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_66 = model66.predict(X_test)\n",
    "y_pred_66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([X_test[\"srch_id\"], pos.iloc[test_inds]], axis=1)\n",
    "df['predictions'] = y_pred_66\n",
    "# df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3733667894875733\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in df['srch_id'].unique():\n",
    "    a1 = [df[df[\"srch_id\"]==i][\"position\"].values]\n",
    "    a2 = [df[df[\"srch_id\"]==i][\"predictions\"].values]\n",
    "    scores.append(ndcg_score(a1, a2, k=5))\n",
    "print(sum(scores)/len(scores)) #3733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model66_ = lgb.LGBMRanker(objective=\"lambdarank\", metric=\"ndcg\", verbose=1, num_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.568007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 52080\n",
      "[LightGBM] [Info] Number of data points in the train set: 4958347, number of used features: 100\n",
      "CPU times: user 8min 1s, sys: 55.6 s, total: 8min 57s\n",
      "Wall time: 3min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRanker(metric='ndcg', num_iterations=200, objective='lambdarank', verbose=1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model66_.fit(train.drop([target], axis=1), -train[target], verbose=1,\n",
    "             group=train['srch_id'].value_counts(sort=False).sort_index(),categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59142269,  1.12019842, -0.25803262, ...,  0.03295955,\n",
       "       -0.05237822,  0.23207979])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_pred66 = model66_.predict(test)\n",
    "y_pred66 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Listwise: LGBMRanker with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=4, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_11 = lgb.LGBMRanker(objective=\"lambdarank\", metric=\"ndcg\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_grid_params = {\n",
    "    'learning_rate': [0.05, 0.1, 0.15], \n",
    "    'n_estimators': [80, 100, 110, 120], \n",
    "    'min_child_samples': [17, 20, 23],\n",
    "    'num_leaves': [28, 31, 34],# large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type': [\"gbdt\", \"dart\", \"goss\"], # for better accuracy -> try dart\n",
    "    'max_bin': [255, 300],#large max_bin helps improve accuracy but might slow down training progress\n",
    "    'subsample': [1, 0.9],\n",
    "    'random_state': [42],\n",
    "    'verbose': [1]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(lgb_11, random_grid_params, n_iter=2, scoring=custom_scorer, cv=gss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 977, in fit\n",
      "    super(LGBMRanker, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 612, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\", line 2053, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\", line 1321, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\", line 1141, in _lazy_init\n",
      "    self.set_group(group)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\", line 1713, in set_group\n",
      "    self.set_field('group', group)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\", line 1485, in set_field\n",
      "    _safe_call(_LIB.LGBM_DatasetSetField(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Sum of query counts is not same with #data\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Label should be non-negative (met -1.000000) for ranking task",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         super(LGBMRanker, self).fit(X, y, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    978\u001b[0m                                     \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[1;32m    613\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2056\u001b[0m             \u001b[0mparams_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0m\u001b[1;32m   2059\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Label should be non-negative (met -1.000000) for ranking task"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "random_search.fit(X_train, y_train, groups=X_train['srch_id'], group=X_train['srch_id'].value_counts(sort=False).sort_index(),\n",
    "                  eval_at=5, categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['prop_country_id', 'prop_id', 'site_id', 'srch_destination_id', 'srch_id', 'visitor_location_country_id']\n",
      "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.034383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54305\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@5: 0.299461\tvalid_1's ndcg@5: 0.292636\n",
      "[2]\ttraining's ndcg@5: 0.339943\tvalid_1's ndcg@5: 0.325278\n",
      "[3]\ttraining's ndcg@5: 0.352922\tvalid_1's ndcg@5: 0.333621\n",
      "[4]\ttraining's ndcg@5: 0.36205\tvalid_1's ndcg@5: 0.340558\n",
      "[5]\ttraining's ndcg@5: 0.367696\tvalid_1's ndcg@5: 0.346121\n",
      "[6]\ttraining's ndcg@5: 0.372135\tvalid_1's ndcg@5: 0.348248\n",
      "[7]\ttraining's ndcg@5: 0.37652\tvalid_1's ndcg@5: 0.351114\n",
      "[8]\ttraining's ndcg@5: 0.379403\tvalid_1's ndcg@5: 0.352944\n",
      "[9]\ttraining's ndcg@5: 0.382513\tvalid_1's ndcg@5: 0.354109\n",
      "[10]\ttraining's ndcg@5: 0.384195\tvalid_1's ndcg@5: 0.356146\n",
      "[11]\ttraining's ndcg@5: 0.387085\tvalid_1's ndcg@5: 0.357555\n",
      "[12]\ttraining's ndcg@5: 0.389873\tvalid_1's ndcg@5: 0.359794\n",
      "[13]\ttraining's ndcg@5: 0.392433\tvalid_1's ndcg@5: 0.360516\n",
      "[14]\ttraining's ndcg@5: 0.393826\tvalid_1's ndcg@5: 0.360487\n",
      "[15]\ttraining's ndcg@5: 0.397919\tvalid_1's ndcg@5: 0.363979\n",
      "[16]\ttraining's ndcg@5: 0.39977\tvalid_1's ndcg@5: 0.365685\n",
      "[17]\ttraining's ndcg@5: 0.400871\tvalid_1's ndcg@5: 0.366001\n",
      "[18]\ttraining's ndcg@5: 0.403141\tvalid_1's ndcg@5: 0.366552\n",
      "[19]\ttraining's ndcg@5: 0.404682\tvalid_1's ndcg@5: 0.366711\n",
      "[20]\ttraining's ndcg@5: 0.406064\tvalid_1's ndcg@5: 0.366995\n",
      "[21]\ttraining's ndcg@5: 0.407294\tvalid_1's ndcg@5: 0.367148\n",
      "[22]\ttraining's ndcg@5: 0.408342\tvalid_1's ndcg@5: 0.367164\n",
      "[23]\ttraining's ndcg@5: 0.41118\tvalid_1's ndcg@5: 0.369475\n",
      "[24]\ttraining's ndcg@5: 0.41214\tvalid_1's ndcg@5: 0.369639\n",
      "[25]\ttraining's ndcg@5: 0.413536\tvalid_1's ndcg@5: 0.37054\n",
      "[26]\ttraining's ndcg@5: 0.415496\tvalid_1's ndcg@5: 0.371322\n",
      "[27]\ttraining's ndcg@5: 0.416467\tvalid_1's ndcg@5: 0.371569\n",
      "[28]\ttraining's ndcg@5: 0.417954\tvalid_1's ndcg@5: 0.371365\n",
      "[29]\ttraining's ndcg@5: 0.419884\tvalid_1's ndcg@5: 0.372244\n",
      "[30]\ttraining's ndcg@5: 0.421168\tvalid_1's ndcg@5: 0.372486\n",
      "[31]\ttraining's ndcg@5: 0.42253\tvalid_1's ndcg@5: 0.373218\n",
      "[32]\ttraining's ndcg@5: 0.424188\tvalid_1's ndcg@5: 0.374424\n",
      "[33]\ttraining's ndcg@5: 0.42489\tvalid_1's ndcg@5: 0.374508\n",
      "[34]\ttraining's ndcg@5: 0.425894\tvalid_1's ndcg@5: 0.375227\n",
      "[35]\ttraining's ndcg@5: 0.426807\tvalid_1's ndcg@5: 0.375707\n",
      "[36]\ttraining's ndcg@5: 0.428073\tvalid_1's ndcg@5: 0.376192\n",
      "[37]\ttraining's ndcg@5: 0.429546\tvalid_1's ndcg@5: 0.375997\n",
      "[38]\ttraining's ndcg@5: 0.430552\tvalid_1's ndcg@5: 0.376615\n",
      "[39]\ttraining's ndcg@5: 0.431312\tvalid_1's ndcg@5: 0.376947\n",
      "[40]\ttraining's ndcg@5: 0.432126\tvalid_1's ndcg@5: 0.376667\n",
      "[41]\ttraining's ndcg@5: 0.432707\tvalid_1's ndcg@5: 0.377008\n",
      "[42]\ttraining's ndcg@5: 0.433896\tvalid_1's ndcg@5: 0.377125\n",
      "[43]\ttraining's ndcg@5: 0.435067\tvalid_1's ndcg@5: 0.377954\n",
      "[44]\ttraining's ndcg@5: 0.436064\tvalid_1's ndcg@5: 0.377682\n",
      "[45]\ttraining's ndcg@5: 0.43691\tvalid_1's ndcg@5: 0.377954\n",
      "[46]\ttraining's ndcg@5: 0.437732\tvalid_1's ndcg@5: 0.37844\n",
      "[47]\ttraining's ndcg@5: 0.43875\tvalid_1's ndcg@5: 0.378274\n",
      "[48]\ttraining's ndcg@5: 0.439632\tvalid_1's ndcg@5: 0.378636\n",
      "[49]\ttraining's ndcg@5: 0.440883\tvalid_1's ndcg@5: 0.379279\n",
      "[50]\ttraining's ndcg@5: 0.44179\tvalid_1's ndcg@5: 0.379186\n",
      "[51]\ttraining's ndcg@5: 0.442398\tvalid_1's ndcg@5: 0.37906\n",
      "[52]\ttraining's ndcg@5: 0.443705\tvalid_1's ndcg@5: 0.379241\n",
      "[53]\ttraining's ndcg@5: 0.444582\tvalid_1's ndcg@5: 0.379286\n",
      "[54]\ttraining's ndcg@5: 0.445039\tvalid_1's ndcg@5: 0.379455\n",
      "[55]\ttraining's ndcg@5: 0.445866\tvalid_1's ndcg@5: 0.380243\n",
      "[56]\ttraining's ndcg@5: 0.447151\tvalid_1's ndcg@5: 0.380137\n",
      "[57]\ttraining's ndcg@5: 0.447876\tvalid_1's ndcg@5: 0.380194\n",
      "[58]\ttraining's ndcg@5: 0.448689\tvalid_1's ndcg@5: 0.380495\n",
      "[59]\ttraining's ndcg@5: 0.449281\tvalid_1's ndcg@5: 0.380738\n",
      "[60]\ttraining's ndcg@5: 0.450461\tvalid_1's ndcg@5: 0.381084\n",
      "[61]\ttraining's ndcg@5: 0.450965\tvalid_1's ndcg@5: 0.380933\n",
      "[62]\ttraining's ndcg@5: 0.451767\tvalid_1's ndcg@5: 0.381039\n",
      "[63]\ttraining's ndcg@5: 0.452842\tvalid_1's ndcg@5: 0.38198\n",
      "[64]\ttraining's ndcg@5: 0.453302\tvalid_1's ndcg@5: 0.381626\n",
      "[65]\ttraining's ndcg@5: 0.453934\tvalid_1's ndcg@5: 0.382116\n",
      "[66]\ttraining's ndcg@5: 0.45461\tvalid_1's ndcg@5: 0.382232\n",
      "[67]\ttraining's ndcg@5: 0.455489\tvalid_1's ndcg@5: 0.382434\n",
      "[68]\ttraining's ndcg@5: 0.456243\tvalid_1's ndcg@5: 0.38243\n",
      "[69]\ttraining's ndcg@5: 0.456791\tvalid_1's ndcg@5: 0.382317\n",
      "[70]\ttraining's ndcg@5: 0.457566\tvalid_1's ndcg@5: 0.382534\n",
      "[71]\ttraining's ndcg@5: 0.458105\tvalid_1's ndcg@5: 0.382585\n",
      "[72]\ttraining's ndcg@5: 0.458509\tvalid_1's ndcg@5: 0.38311\n",
      "[73]\ttraining's ndcg@5: 0.459384\tvalid_1's ndcg@5: 0.382916\n",
      "[74]\ttraining's ndcg@5: 0.460413\tvalid_1's ndcg@5: 0.382991\n",
      "[75]\ttraining's ndcg@5: 0.461082\tvalid_1's ndcg@5: 0.383133\n",
      "[76]\ttraining's ndcg@5: 0.461534\tvalid_1's ndcg@5: 0.382778\n",
      "[77]\ttraining's ndcg@5: 0.462008\tvalid_1's ndcg@5: 0.383118\n",
      "[78]\ttraining's ndcg@5: 0.463065\tvalid_1's ndcg@5: 0.383748\n",
      "[79]\ttraining's ndcg@5: 0.463634\tvalid_1's ndcg@5: 0.384151\n",
      "[80]\ttraining's ndcg@5: 0.464354\tvalid_1's ndcg@5: 0.384122\n",
      "[81]\ttraining's ndcg@5: 0.464802\tvalid_1's ndcg@5: 0.384137\n",
      "[82]\ttraining's ndcg@5: 0.465441\tvalid_1's ndcg@5: 0.384113\n",
      "[83]\ttraining's ndcg@5: 0.466152\tvalid_1's ndcg@5: 0.384359\n",
      "[84]\ttraining's ndcg@5: 0.466867\tvalid_1's ndcg@5: 0.384906\n",
      "[85]\ttraining's ndcg@5: 0.467511\tvalid_1's ndcg@5: 0.385112\n",
      "[86]\ttraining's ndcg@5: 0.46786\tvalid_1's ndcg@5: 0.38507\n",
      "[87]\ttraining's ndcg@5: 0.468556\tvalid_1's ndcg@5: 0.385192\n",
      "[88]\ttraining's ndcg@5: 0.469232\tvalid_1's ndcg@5: 0.384946\n",
      "[89]\ttraining's ndcg@5: 0.469819\tvalid_1's ndcg@5: 0.385385\n",
      "[90]\ttraining's ndcg@5: 0.470432\tvalid_1's ndcg@5: 0.385679\n",
      "[91]\ttraining's ndcg@5: 0.470858\tvalid_1's ndcg@5: 0.385439\n",
      "[92]\ttraining's ndcg@5: 0.471585\tvalid_1's ndcg@5: 0.385524\n",
      "[93]\ttraining's ndcg@5: 0.472212\tvalid_1's ndcg@5: 0.385662\n",
      "[94]\ttraining's ndcg@5: 0.47291\tvalid_1's ndcg@5: 0.385421\n",
      "[95]\ttraining's ndcg@5: 0.473226\tvalid_1's ndcg@5: 0.385519\n",
      "[96]\ttraining's ndcg@5: 0.473866\tvalid_1's ndcg@5: 0.385753\n",
      "[97]\ttraining's ndcg@5: 0.474551\tvalid_1's ndcg@5: 0.385311\n",
      "[98]\ttraining's ndcg@5: 0.474959\tvalid_1's ndcg@5: 0.385523\n",
      "[99]\ttraining's ndcg@5: 0.475746\tvalid_1's ndcg@5: 0.385946\n",
      "[100]\ttraining's ndcg@5: 0.476284\tvalid_1's ndcg@5: 0.385938\n",
      "CPU times: user 4min 55s, sys: 32.1 s, total: 5min 27s\n",
      "Wall time: 2min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRanker(metric='ndcg', objective='lambdarank', verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model7.fit(X_train, y_train_, eval_set=[(X_train, y_train_), (X_test, y_test_)], eval_group=[X_train['srch_id'].value_counts(sort=False).sort_index(), X_test['srch_id'].value_counts(sort=False).sort_index()], group=X_train['srch_id'].value_counts(sort=False).sort_index(),\n",
    "            eval_at=5,categorical_feature=rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46175809, -0.7501605 , -0.54465082, ..., -0.26479785,\n",
       "        0.05585781,  0.35997866])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_7 = model7.predict(X_test)\n",
    "y_pred_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>target_score</th>\n",
       "      <th>predictions</th>\n",
       "      <th>predictions_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.461758</td>\n",
       "      <td>0.506286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.750161</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.544651</td>\n",
       "      <td>0.357319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.536805</td>\n",
       "      <td>0.358480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.192074</td>\n",
       "      <td>0.466368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.639114</td>\n",
       "      <td>0.343336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.658146</td>\n",
       "      <td>0.340519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.320207</td>\n",
       "      <td>0.390541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.696013</td>\n",
       "      <td>0.334914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.215528</td>\n",
       "      <td>0.406035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.704841</td>\n",
       "      <td>0.333608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329605</td>\n",
       "      <td>0.486725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661831</td>\n",
       "      <td>0.339974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.515708</td>\n",
       "      <td>0.514272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400357</td>\n",
       "      <td>0.497198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.743673</td>\n",
       "      <td>0.327860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.679548</td>\n",
       "      <td>0.337352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.231557</td>\n",
       "      <td>0.403663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.413938</td>\n",
       "      <td>0.376667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.413938</td>\n",
       "      <td>0.376667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.544651</td>\n",
       "      <td>0.357319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.530346</td>\n",
       "      <td>0.359436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.760235</td>\n",
       "      <td>0.550466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.255554</td>\n",
       "      <td>0.400111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.262295</td>\n",
       "      <td>0.399113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.432175</td>\n",
       "      <td>0.373967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321054</td>\n",
       "      <td>0.485459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.725411</td>\n",
       "      <td>0.330563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.267856</td>\n",
       "      <td>0.398290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.253260</td>\n",
       "      <td>0.252431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     srch_id  target_score  predictions  predictions_n\n",
       "119       12             0     0.461758       0.506286\n",
       "120       12             0    -0.750161       0.326900\n",
       "121       12             0    -0.544651       0.357319\n",
       "122       12             0    -0.536805       0.358480\n",
       "123       12             0     0.192074       0.466368\n",
       "124       12             0    -0.639114       0.343336\n",
       "125       12             0    -0.658146       0.340519\n",
       "126       12             0    -0.320207       0.390541\n",
       "127       12             0    -0.696013       0.334914\n",
       "128       12             0    -0.215528       0.406035\n",
       "129       12             0    -0.704841       0.333608\n",
       "130       12             0     0.329605       0.486725\n",
       "131       12             0    -0.661831       0.339974\n",
       "132       12             0     0.515708       0.514272\n",
       "133       12             0     0.400357       0.497198\n",
       "134       12             0    -0.743673       0.327860\n",
       "135       12             0    -0.679548       0.337352\n",
       "136       12             0    -0.231557       0.403663\n",
       "137       12             0    -0.413938       0.376667\n",
       "138       12             0    -0.413938       0.376667\n",
       "139       12             0    -0.544651       0.357319\n",
       "140       12             0    -0.530346       0.359436\n",
       "141       12             0     0.760235       0.550466\n",
       "142       12             0    -0.255554       0.400111\n",
       "143       12             0    -0.262295       0.399113\n",
       "144       12             0    -0.432175       0.373967\n",
       "145       12             1     0.321054       0.485459\n",
       "146       12             0    -0.725411       0.330563\n",
       "207       25             0    -0.267856       0.398290\n",
       "208       25             0    -1.253260       0.252431"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X_test[\"srch_id\"], y_test_], axis=1)\n",
    "df['predictions'] = y_pred_7\n",
    "df['predictions_n'] = (df['predictions']-df['predictions'].min())/(df['predictions'].max()-df['predictions'].min())\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7919465816683966\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in df['srch_id'].unique():\n",
    "#     #t1\n",
    "#     a1 = [df[df[\"srch_id\"]==i][\"target_score\"].values]\n",
    "#     a2 = [df[df[\"srch_id\"]==i][\"predictions\"].values]\n",
    "#     scores.append(ndcg_score(a1, a2, k=5))\n",
    "\n",
    "    #t2 - better so far 0,5\n",
    "    a1 = df[df[\"srch_id\"]==i][\"target_score\"].values\n",
    "    a2 = df[df[\"srch_id\"]==i][\"predictions_n\"].values\n",
    "    scores.append(ndcg5(a1, a2))\n",
    "print(sum(scores)/len(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46165524,  0.88704194, -0.1051241 , ...,  0.01261689,\n",
       "        0.06631362,  0.17210367])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred7 = model7.predict(test)\n",
    "y_pred7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by target_score (click_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(data = -y_pred66, columns=['target_score'])\n",
    "# r.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[\"srch_id\"] = test['srch_id']\n",
    "r[\"prop_id\"] = test['prop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = r.sort_values(['srch_id','target_score'])[[\"srch_id\",\"prop_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"sub13.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tCatBoost https://colab.research.google.com/drive/1cuFTgBFRVFD8dVP74QkhNZ_9v7sDgx_z \n",
    "\n",
    "https://www.kaggle.com/code/danofer/catboost-ranking-ncdg-expedia-search-queries \n",
    "-\tTF listwise https://www.tensorflow.org/ranking "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
